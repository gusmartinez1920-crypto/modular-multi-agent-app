# üß† Sistema Modular de Multiagentes (MMAS) para An√°lise de Documentos

## üéØ Vis√£o Geral do Projeto

Este projeto demonstra uma **Arquitetura Orientada a Agentes (AOA)** utilizando o padr√£o **RAG (Retrieval Augmented Generation)** para processar, analisar e responder a consultas sobre documentos n√£o estruturados (PDFs).

O principal objetivo √© substituir um processamento sequencial simples por uma orquestra√ß√£o modular, onde cada etapa (extra√ß√£o, mem√≥ria, an√°lise) √© tratada por um **Agente de IA especializado**.

**Recursos Chave:**
* **Modularidade:** Separa√ß√£o de responsabilidades em microsservi√ßos (Frontend, API, Backend de Agentes).
* **Rastreabilidade:** Uso de um `CoordinatorAgent` e logs detalhados (via `task_id`) para monitorar o fluxo de trabalho.
* **RAG Integrado:** Utiliza√ß√£o de um Vector DB (ChromaDB) para enriquecer o contexto do LLM.
* **Tecnologia:** Backend Python (FastAPI, Agentes), Frontend (Next.js) e LLM (Google Gemini).

## üèõÔ∏è Arquitetura do Sistema

A solu√ß√£o √© dividida em tr√™s cont√™ineres Docker principais, gerenciados pelo `docker-compose`. 

| Servi√ßo | Tecnologia | Fun√ß√£o Principal | Porta Exposta |
| :--- | :--- | :--- | :--- |
| **Frontend** | Next.js (React) | Interface de upload e visualiza√ß√£o de resultados. | `3000` |
| **API Gateway** | FastAPI | Recebimento de requisi√ß√µes HTTP e roteamento para o motor de agentes. | `8000` |
| **Agent Backend** | Python (Agentes) | Orquestra√ß√£o do Workflow e execu√ß√£o da l√≥gica de IA/RAG. | *(Nenhuma)* |

### üîç O Ciclo de Vida do Agente

O cora√ß√£o do sistema √© o `Agent Backend`, que executa o fluxo definido no arquivo YAML:

1.  **`CoordinatorAgent`**: Carrega o workflow (`default_pdf_analysis.yaml`) e dita a ordem de execu√ß√£o.
2.  **`ExtractionAgent`**: Utiliza a `PDFReaderTool` para transformar o PDF em *chunks* de texto.
3.  **`MemoryAgent`**: Gerencia a base de conhecimento. Ele armazena os novos *chunks* no Vector DB e executa a busca RAG para recuperar o conhecimento relevante de documentos passados/outros.
4.  **`AnalysisAgent` (Gemini)**: Recebe o prompt do usu√°rio + todos os *chunks* de contexto. Ele utiliza a `LLMTool` (SDK do Gemini) para raciocinar e gerar a resposta final.
5.  **`DeliveryAgent`**: Formata a resposta final em um padr√£o JSON limpo para o sistema externo.

## üõ†Ô∏è Guia de Instala√ß√£o e Execu√ß√£o

### Pr√©-requisitos

* **Docker e Docker Compose:** Essenciais para rodar a arquitetura de microsservi√ßos.
* **Chave da API do Gemini:** Necess√°ria para o `AnalysisAgent` funcionar.

### Passo 1: Configurar Vari√°veis de Ambiente (Chave Secreta)

Crie um arquivo chamado **`.env`** na **raiz do seu projeto**. Este arquivo fornece as chaves e configura√ß√µes para o Docker e para o Backend de Agentes.

**ATEN√á√ÉO:** O arquivo `.env` √© listado no `.gitignore` e **N√ÉO DEVE** ser enviado ao GitHub.

```env
# ARQUIVO: .env (na raiz do projeto)

# Chave da API: Substitua pela sua chave real do Google AI Studio.
GEMINI_API_KEY="SUA_CHAVE_DA_API_GEMINI_AQUI" 

# Modelo do LLM a ser usado pelo AnalysisAgent
LLM_MODEL="gemini-2.5-flash"

# N√≠vel de Log (Use DEBUG para ver o fluxo detalhado dos agentes)
LOG_LEVEL="DEBUG"